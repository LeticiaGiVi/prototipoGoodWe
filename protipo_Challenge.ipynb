{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0CDCXcohWg8W"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import base64\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, date, timedelta, time as dtime\n",
        "from typing import Literal, Dict, Any, List, Optional"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Region = Literal[\"us\", \"eu\"]\n",
        "\n",
        "class SEMSClient:\n",
        "    BASE_URLS = {\n",
        "        \"us\": \"https://us.semsportal.com\",\n",
        "        \"eu\": \"https://eu.semsportal.com\",\n",
        "    }\n",
        "\n",
        "    def __init__(self, account: str, password: str, login_region: Region = \"us\", data_region: Region = \"eu\"):\n",
        "        self.account = account\n",
        "        self.password = password\n",
        "        self.login_region = login_region\n",
        "        self.data_region = data_region\n",
        "        self._token = None\n",
        "\n",
        "    def _initial_token(self) -> str:\n",
        "        original = {\n",
        "            \"uid\": \"\",\n",
        "            \"timestamp\": 0,\n",
        "            \"token\": \"\",\n",
        "            \"client\": \"web\",\n",
        "            \"version\": \"\",\n",
        "            \"language\": \"en\"\n",
        "        }\n",
        "        b = json.dumps(original).encode(\"utf-8\")\n",
        "        return base64.b64encode(b).decode(\"utf-8\")\n",
        "\n",
        "    def login(self) -> str:\n",
        "        \"\"\"Faz o crosslogin e armazena o token\"\"\"\n",
        "        url = f\"{self.BASE_URLS[self.login_region]}/api/v2/common/crosslogin\"\n",
        "        headers = {\n",
        "            \"Token\": self._initial_token(),\n",
        "            \"Content-Type\": \"application/json\",\n",
        "            \"Accept\": \"*/*\"\n",
        "        }\n",
        "        payload = {\n",
        "            \"account\": self.account,\n",
        "            \"pwd\": self.password,\n",
        "            \"agreement_agreement\": 0,\n",
        "            \"is_local\": False\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.post(url, json=payload, headers=headers, timeout=30)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            js = response.json()\n",
        "            if \"data\" not in js or js.get(\"code\") not in (0, 1, 200):\n",
        "                raise RuntimeError(f\"Login falhou: {js}\")\n",
        "\n",
        "            data_to_string = json.dumps(js[\"data\"])\n",
        "            self._token = base64.b64encode(data_to_string.encode(\"utf-8\")).decode(\"utf-8\")\n",
        "            return self._token\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            raise RuntimeError(f\"Erro de conexão durante login: {e}\")\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Erro durante login: {e}\")\n",
        "\n",
        "    def get_inverter_data_by_column(self, inverter_sn: str, column: str, date_str: str) -> Dict[str, Any]:\n",
        "        if not self._token:\n",
        "            raise RuntimeError(\"Token não disponível. Execute login() primeiro.\")\n",
        "\n",
        "        url = f\"{self.BASE_URLS[self.data_region]}/api/PowerStationMonitor/GetInverterDataByColumn\"\n",
        "        headers = {\n",
        "            \"Token\": self._token,\n",
        "            \"Content-Type\": \"application/json\",\n",
        "            \"Accept\": \"*/*\"\n",
        "        }\n",
        "        payload = {\n",
        "            \"date\": date_str,\n",
        "            \"column\": column,\n",
        "            \"id\": inverter_sn\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.post(url, json=payload, headers=headers, timeout=30)\n",
        "            response.raise_for_status()\n",
        "            return response.json()\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            raise RuntimeError(f\"Erro de conexão ao buscar dados: {e}\")\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Erro ao buscar dados: {e}\")\n",
        "\n",
        "class SEMSDataProcessor:\n",
        "    @staticmethod\n",
        "    def parse_column_timeseries(resp_json: Dict[str, Any], column_name: str) -> pd.DataFrame:\n",
        "\n",
        "        def _parse_time(ts):\n",
        "            v = pd.to_datetime(ts, errors='coerce')\n",
        "            if pd.isna(v):\n",
        "                try:\n",
        "                    v = pd.to_datetime(ts, dayfirst=True, errors='coerce')\n",
        "                except:\n",
        "                    v = pd.NaT\n",
        "            return v\n",
        "\n",
        "        items = []\n",
        "        if isinstance(resp_json, dict):\n",
        "            data_obj = resp_json.get('data')\n",
        "            if isinstance(data_obj, dict):\n",
        "                for key in ('column1', 'column2', 'column3', 'items', 'list', 'datas', 'result'):\n",
        "                    if key in data_obj and isinstance(data_obj[key], list):\n",
        "                        items = data_obj[key]\n",
        "                        break\n",
        "\n",
        "            if not items:\n",
        "                for key in ('data', 'items', 'list', 'result', 'datas'):\n",
        "                    if key in resp_json and isinstance(resp_json[key], list):\n",
        "                        items = resp_json[key]\n",
        "                        break\n",
        "\n",
        "        if not items:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        times, values = [], []\n",
        "        for item in items:\n",
        "            if not isinstance(item, dict):\n",
        "                continue\n",
        "\n",
        "            timestamp = (item.get('time') or\n",
        "                        item.get('date') or\n",
        "                        item.get('collectTime') or\n",
        "                        item.get('cTime') or\n",
        "                        item.get('tm'))\n",
        "\n",
        "            if column_name in item:\n",
        "                value = item.get(column_name)\n",
        "            else:\n",
        "                value = (item.get('value') or\n",
        "                        item.get('v') or\n",
        "                        item.get('val') or\n",
        "                        item.get('column'))\n",
        "\n",
        "            if timestamp is None or value is None:\n",
        "                continue\n",
        "\n",
        "            parsed_time = _parse_time(timestamp)\n",
        "            if pd.isna(parsed_time):\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                parsed_value = float(str(value).replace(',', '.'))\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "            times.append(parsed_time)\n",
        "            values.append(parsed_value)\n",
        "\n",
        "        if not times:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        df = pd.DataFrame({'time': times, column_name: values})\n",
        "        return df.dropna().sort_values('time').reset_index(drop=True)\n",
        "\n",
        "\n",
        "def fetch_weekly_sems_data(account: str,\n",
        "                          password: str,\n",
        "                          inverter_sn: str,\n",
        "                          start_date: date,\n",
        "                          columns: List[str],\n",
        "                          login_region: Region = \"us\",\n",
        "                          data_region: Region = \"eu\") -> pd.DataFrame:\n",
        "\n",
        "    print(f\"Buscando dados da semana começando em {start_date}\")\n",
        "\n",
        "    client = SEMSClient(account, password, login_region, data_region)\n",
        "    processor = SEMSDataProcessor()\n",
        "\n",
        "    try:\n",
        "        client.login()\n",
        "        print(\"Login realizado com sucesso!\")\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Erro no login: {e}\")\n",
        "\n",
        "    weekly_data = []\n",
        "\n",
        "    for day_offset in range(7):\n",
        "        current_date = start_date + timedelta(days=day_offset)\n",
        "        date_str = datetime.combine(current_date, dtime(0, 0)).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "        print(f\"\\nDia {day_offset + 1}/7: {current_date}\")\n",
        "\n",
        "        day_dataframes = []\n",
        "        for column in columns:\n",
        "            try:\n",
        "                response = client.get_inverter_data_by_column(inverter_sn, column, date_str)\n",
        "                df_column = processor.parse_column_timeseries(response, column)\n",
        "\n",
        "                if not df_column.empty:\n",
        "                    day_dataframes.append(df_column)\n",
        "                    print(f\"  {column}: {len(df_column)} registros\")\n",
        "                else:\n",
        "                    print(f\"  {column}: sem dados\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  Erro em {column}: {e}\")\n",
        "\n",
        "        if day_dataframes:\n",
        "            day_df = day_dataframes[0]\n",
        "            for df_next in day_dataframes[1:]:\n",
        "                day_df = pd.merge_asof(\n",
        "                    day_df.sort_values(\"time\"),\n",
        "                    df_next.sort_values(\"time\"),\n",
        "                    on=\"time\",\n",
        "                    direction=\"nearest\"\n",
        "                )\n",
        "            weekly_data.append(day_df)\n",
        "\n",
        "    if weekly_data:\n",
        "        final_df = pd.concat(weekly_data, ignore_index=True)\n",
        "        final_df = final_df.sort_values(\"time\").reset_index(drop=True)\n",
        "        print(f\"\\nDados semanais combinados: {len(final_df)} registros\")\n",
        "        return final_df\n",
        "    else:\n",
        "        print(\"Nenhum dado encontrado para a semana!\")\n",
        "        return pd.DataFrame()"
      ],
      "metadata": {
        "id": "M5orqnr-x9XX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPAjYUQ_DvZx",
        "outputId": "8531bfcd-cd03-4ee4-90b6-cdae2b5948da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Buscando dados da semana começando em 2025-09-29\n",
            "Login realizado com sucesso!\n",
            "\n",
            "Dia 1/7: 2025-09-29\n",
            "  Pac: sem dados\n",
            "  Eday: sem dados\n",
            "  Cbattery1: sem dados\n",
            "  Vbattery1: sem dados\n",
            "  Ipv1: sem dados\n",
            "  Ipv2: sem dados\n",
            "  Vpv1: sem dados\n",
            "  Vpv1: sem dados\n",
            "\n",
            "Dia 2/7: 2025-09-30\n",
            "  Pac: sem dados\n",
            "  Eday: sem dados\n",
            "  Cbattery1: sem dados\n",
            "  Vbattery1: sem dados\n",
            "  Ipv1: sem dados\n",
            "  Ipv2: sem dados\n",
            "  Vpv1: sem dados\n",
            "  Vpv1: sem dados\n",
            "\n",
            "Dia 3/7: 2025-10-01\n",
            "  Pac: sem dados\n",
            "  Eday: sem dados\n",
            "  Cbattery1: sem dados\n",
            "  Vbattery1: sem dados\n",
            "  Ipv1: sem dados\n",
            "  Ipv2: sem dados\n",
            "  Vpv1: sem dados\n",
            "  Vpv1: sem dados\n",
            "\n",
            "Dia 4/7: 2025-10-02\n",
            "  Pac: sem dados\n",
            "  Eday: sem dados\n",
            "  Cbattery1: sem dados\n",
            "  Vbattery1: sem dados\n",
            "  Ipv1: sem dados\n",
            "  Ipv2: sem dados\n",
            "  Vpv1: sem dados\n",
            "  Vpv1: sem dados\n",
            "\n",
            "Dia 5/7: 2025-10-03\n",
            "  Pac: sem dados\n",
            "  Eday: sem dados\n",
            "  Cbattery1: sem dados\n",
            "  Vbattery1: sem dados\n",
            "  Ipv1: sem dados\n",
            "  Ipv2: sem dados\n",
            "  Vpv1: sem dados\n",
            "  Vpv1: sem dados\n",
            "\n",
            "Dia 6/7: 2025-10-04\n",
            "  Pac: sem dados\n",
            "  Eday: sem dados\n",
            "  Cbattery1: sem dados\n",
            "  Vbattery1: sem dados\n",
            "  Ipv1: sem dados\n",
            "  Ipv2: sem dados\n",
            "  Vpv1: sem dados\n",
            "  Vpv1: sem dados\n",
            "\n",
            "Dia 7/7: 2025-10-05\n",
            "  Pac: sem dados\n",
            "  Eday: sem dados\n",
            "  Cbattery1: sem dados\n",
            "  Vbattery1: sem dados\n",
            "  Ipv1: sem dados\n",
            "  Ipv2: sem dados\n",
            "  Vpv1: sem dados\n",
            "  Vpv1: sem dados\n",
            "Nenhum dado encontrado para a semana!\n",
            "Nenhum dado foi encontrado!\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    ACCOUNT = os.getenv(\"SEMS_ACCOUNT\", \" 1ccrgirls@gmail.com\")\n",
        "    PASSWORD = os.getenv(\"SEMS_PASSWORD\", \"Goodwe2018\")\n",
        "    INVERTER_SN = \"53600ERN238W0001\"\n",
        "    START_DATE = date(2025, 9, 29)\n",
        "    #COLUMNS = [\"PMeter\", \"PVGeneration\", \"SOC\",\"VMPPT1\",\"VMPPT2\",\"IMPPT1\",\"IMPPT2\"]\n",
        "    COLUMNS = [\"Pac\", \"Eday\", \"Cbattery1\",\"Vbattery1\",\"Ipv1\",\"Ipv2\",\"Vpv1\", \"Vpv1\"]\n",
        "\n",
        "    try:\n",
        "        df = fetch_weekly_sems_data(\n",
        "            account=ACCOUNT,\n",
        "            password=PASSWORD,\n",
        "            inverter_sn=INVERTER_SN,\n",
        "            start_date=START_DATE,\n",
        "            columns=COLUMNS,\n",
        "            login_region=\"us\",\n",
        "            data_region=\"eu\"\n",
        "        )\n",
        "\n",
        "        if not df.empty:\n",
        "            print(f\"\\n=== DADOS DA SEMANA ===\")\n",
        "            print(f\"Período: {df['time'].min()} até {df['time'].max()}\")\n",
        "            print(f\"Total de registros: {len(df)}\")\n",
        "            print(f\"Colunas: {list(df.columns)}\")\n",
        "\n",
        "            filename = f\"sems_weekly_{INVERTER_SN}_{START_DATE.strftime('%Y%m%d')}_week.csv\"\n",
        "            df.to_csv(filename, index=False)\n",
        "            print(f\"\\nDados salvos em: {filename}\")\n",
        "\n",
        "            df['date'] = df['time'].dt.date\n",
        "            daily_stats = df.groupby('date').agg({\n",
        "                'Pac': ['mean', 'max', 'sum'],\n",
        "                'Eday': 'max'\n",
        "            }).round(2)\n",
        "\n",
        "            print(\"\\n=== ESTATÍSTICAS DIÁRIAS ===\")\n",
        "            print(daily_stats)\n",
        "\n",
        "        else:\n",
        "            print(\"Nenhum dado foi encontrado!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Erro: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kog67chTOG9d"
      },
      "outputs": [],
      "source": [
        "df_energy = pd.read_csv('/content/sems_weekly_5010KETU229W6177_20250929_week.csv')\n",
        "df_energy.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pac = Potência Ativa de Saída (AC) - A potência real que o inversor está entregando para a rede em Watts (W)\n",
        "\n",
        "Eday = Energia Gerada no Dia - Total de energia produzida no dia em Watt-hora (Wh) ou kWh\n",
        "\n",
        "Cbattery1 = Corrente da Bateria- Fluxo de corrente da bateria (positivo = carregando, negativo = descarregando) em Amperes (A)\n",
        "\n",
        "Vbattery1 = Tensão da Bateria 1 - Voltagem da bateria em Volts (V)\n",
        "\n",
        "Ipv1 e Ipv2\t= Tensão 1 - Voltagem da bateria em Volts (V), Cada \"Ipv\" representa um conjunto de painéis conectados em série\n",
        "\n",
        "Vpv1 e Vpv2\t= Tensão de cada string fotovoltaica em Volts (V)"
      ],
      "metadata": {
        "id": "xYSTjb9wC8Hz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYaJW14JYBEq"
      },
      "outputs": [],
      "source": [
        "df_consumption = pd.read_csv('/content/household_power_consumption17_12_06-23_12_06.csv', parse_dates=['DateTime'])\n",
        "df_consumption.drop('Date', axis=1, inplace=True)\n",
        "df_consumption.drop('DateTime', axis=1, inplace=True)\n",
        "df_consumption['time'] = df_energy['time']\n",
        "df_consumption.drop('Time', axis=1, inplace=True)\n",
        "df_consumption['Sub1_on'] = 0\n",
        "df_consumption['Sub2_on'] = 0\n",
        "df_consumption['Sub3_on'] = 0\n",
        "df_consumption.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Global_active_power =  potência ativa média por minuto das residências (em quilowatts)\n",
        "\n",
        "Global_reactive_power = potência reativa média por minuto das residências (em quilowatts)\n",
        "\n",
        "Voltage = tensão média por minuto (em volts)\n",
        "\n",
        "Global_intensity = intensidade de corrente média por minuto global doméstica (em ampères)\n",
        "\n",
        "Sub_metering_1 = Corresponde à cozinha, contendo principalmente uma máquina de lavar louça, um forno e um micro-ondas. (em watts-hora de energia ativa)\n",
        "\n",
        "Sub_metering_2 = Corresponde à lavanderia, contendo uma máquina de lavar, uma secadora, uma geladeira e uma luz.(em watts-hora de energia ativa)\n",
        "\n",
        "Sub_metering_3 = Corresponde a um aquecedor elétrico de água e um ar condicionado. (em watts-hora de energia ativa)"
      ],
      "metadata": {
        "id": "4Ha8YUZ58e-P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Xn8D3sB67OM"
      },
      "outputs": [],
      "source": [
        "df_energy['Pdc'] = (df_energy['Ipv1'] * df_energy['Vpv1']) + (df_energy['Ipv2'] * df_energy['Vpv2'])\n",
        "df_energy['time'] = pd.to_datetime(df_energy['time']) # Convert to datetime objects\n",
        "df_energy['time_diff_h'] = df_energy['time'].diff().dt.total_seconds().div(3600).fillna(0)\n",
        "df_energy['Ecalc_Wh'] = df_energy['Pdc'] * df_energy['time_diff_h']  # Wh no intervalo\n",
        "\n",
        "df_energy[\"week_day\"] = df_energy[\"time\"].dt.day_name()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pdc = Potência elétrica no lado CC (corrente contínua) dos painéis solares.\n",
        "\n",
        "time_diff_h = tempo entre um dado e outro\n",
        "\n",
        "Ecalc_Wh = quanta energia foi gerada no intervalo a cada 5 minutos (Wh)."
      ],
      "metadata": {
        "id": "p_wXbg4METMW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFFfXwSW7JbV"
      },
      "outputs": [],
      "source": [
        "df_energy.head(200)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def combinar_bancos_dados(arquivo1, arquivo2):\n",
        "\n",
        "    # Ensure 'time' column is datetime type in both dataframes\n",
        "    arquivo1['time'] = pd.to_datetime(arquivo1['time'])\n",
        "    arquivo2['time'] = pd.to_datetime(arquivo2['time'])\n",
        "\n",
        "    # Selecionar colunas específicas (ajuste conforme suas necessidades)\n",
        "    colunas_selecionadas_df1 = ['time','week_day', 'Pac', 'Eday', 'Cbattery1','Vbattery1','Pdc','Ecalc_Wh']\n",
        "    colunas_selecionadas_df2 = ['time', 'Global_active_power', 'Global_reactive_power', 'Voltage','Global_intensity','Sub_metering_1','Sub_metering_2','Sub_metering_3', 'Sub1_on','Sub2_on','Sub3_on']\n",
        "\n",
        "    # Filtrar colunas\n",
        "    df1_filtrado = arquivo1[colunas_selecionadas_df1]\n",
        "    df2_filtrado = arquivo2[colunas_selecionadas_df2]\n",
        "\n",
        "    # Combinar os dados usando a coluna 'time'\n",
        "    df_combinado = pd.merge(df1_filtrado, df2_filtrado, on='time', how='inner')\n",
        "\n",
        "    # Salvar resultado\n",
        "    filename = \"dados_combinados.csv\"\n",
        "    df_combinado.to_csv(filename, index=False)\n",
        "    print(f\"Arquivo '{filename}' criado com sucesso!\")\n",
        "    print(f\"Total de registros: {len(df_combinado)}\")\n",
        "\n",
        "    display(df_combinado.head())\n",
        "\n",
        "# Usar a função\n",
        "combinar_bancos_dados(df_energy, df_consumption)"
      ],
      "metadata": {
        "id": "pNb-QHkwiBzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_combinado = pd.read_csv('/content/dados_combinados.csv')"
      ],
      "metadata": {
        "id": "1D8BgTcHsBrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install influxdb3-python"
      ],
      "metadata": {
        "id": "41ZT_JVewKhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install influxdb-client"
      ],
      "metadata": {
        "id": "mudQoZgFsi9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install influxdb_client influxdb3-python requests pandas"
      ],
      "metadata": {
        "id": "eMnuY2f0XV9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, time\n",
        "from influxdb_client_3 import InfluxDBClient3, Point\n",
        "from influxdb_client import InfluxDBClient\n",
        "\n",
        "\n",
        "bucket = \"projetochallenge\"\n",
        "token = os.environ.get(\"h6nAFNWUsFAE_sWEEZvSq76e_6huFuPMgCbIbsBH7YE2QtGxbKEY3mkC2o0jeobv\")\n",
        "org = \"projetoChallenge\"\n",
        "host = \"https://us-east-1-1.aws.cloud2.influxdata.com\"\n",
        "\n",
        "client = InfluxDBClient3(host=host, token=token, org=org)"
      ],
      "metadata": {
        "id": "Xgiiy2ZKwRWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def enviar_para_influxdb(df_combinado, token, host, org, bucket):\n",
        "    \"\"\"Envia dados para InfluxDB usando influxdb_client_3\"\"\"\n",
        "    try:\n",
        "        from influxdb_client_3 import InfluxDBClient3, Point, WriteOptions\n",
        "\n",
        "        client = InfluxDBClient3(host=host, token=token, org=org, database=bucket)\n",
        "\n",
        "        # Converter time para datetime\n",
        "        df_combinado['time'] = pd.to_datetime(df_combinado['time'])\n",
        "\n",
        "        print(f\"Enviando {len(df_combinado)} registros para InfluxDB...\")\n",
        "\n",
        "        # InfluxDBClient3 usa write() diretamente com DataFrames\n",
        "        client.write(\n",
        "            record=df_combinado,\n",
        "            data_frame_measurement_name='Ecalc_Wh',\n",
        "            data_frame_tag_columns=['week_day', 'Sub1_on', 'Sub2_on', 'Sub3_on'],\n",
        "            data_frame_timestamp_column='time'\n",
        "        )\n",
        "\n",
        "        print(\"Dados importados para InfluxDB com sucesso!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao enviar para InfluxDB: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "    finally:\n",
        "        if 'client' in locals():\n",
        "            client.close()\n"
      ],
      "metadata": {
        "id": "Ri2SeJHAXtCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token = os.getenv(\"INFLUXDB_TOKEN\")\n",
        "os.environ['INFLUXDB_TOKEN'] = 'gnqQ6h_WnjJlv1AtsXA9gVO8eDQDEhwTKvvGUJwhX-RMlpSRMUm9747tNpGpDPsgIQkl9-Q26dHy5kvdq4RIiA=='\n",
        "os.environ['INFLUXDB_BUCKET'] = 'projetoChallenge'\n",
        "if token:\n",
        "    try:\n",
        "        enviar_para_influxdb(\n",
        "            df_combinado=df_combinado,\n",
        "            token=token,\n",
        "            host=\"https://us-east-1-1.aws.cloud2.influxdata.com\",\n",
        "            org=\"4211097531c5837f\",\n",
        "            bucket=\"Prototipo Goodwe\"\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Não foi possível enviar para InfluxDB: {e}\")\n",
        "else:\n",
        "    print(\"Token do InfluxDB não configurado. Pulando envio.\")\n",
        "    print(\"Para configurar: os.environ['INFLUXDB_TOKEN'] = 'seu_token'\")"
      ],
      "metadata": {
        "id": "vaKQK9FOX6Ma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "import os\n",
        "import json\n",
        "import base64\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, date, timedelta, time as dtime\n",
        "from typing import Literal, Dict, Any, List, Optional\n",
        "\n",
        "Region = Literal[\"us\", \"eu\"]\n",
        "\n",
        "class SEMSClient:\n",
        "    BASE_URLS = {\n",
        "        \"us\": \"https://us.semsportal.com\",\n",
        "        \"eu\": \"https://eu.semsportal.com\",\n",
        "    }\n",
        "\n",
        "    def __init__(self, account: str, password: str, login_region: Region = \"us\", data_region: Region = \"eu\"):\n",
        "        self.account = account\n",
        "        self.password = password\n",
        "        self.login_region = login_region\n",
        "        self.data_region = data_region\n",
        "        self._token = None\n",
        "\n",
        "    def _initial_token(self) -> str:\n",
        "        original = {\n",
        "            \"uid\": \"\",\n",
        "            \"timestamp\": 0,\n",
        "            \"token\": \"\",\n",
        "            \"client\": \"web\",\n",
        "            \"version\": \"\",\n",
        "            \"language\": \"en\"\n",
        "        }\n",
        "        b = json.dumps(original).encode(\"utf-8\")\n",
        "        return base64.b64encode(b).decode(\"utf-8\")\n",
        "\n",
        "    def login(self) -> str:\n",
        "        url = f\"{self.BASE_URLS[self.login_region]}/api/v2/common/crosslogin\"\n",
        "        headers = {\n",
        "            \"Token\": self._initial_token(),\n",
        "            \"Content-Type\": \"application/json\",\n",
        "            \"Accept\": \"*/*\"\n",
        "        }\n",
        "        payload = {\n",
        "            \"account\": self.account,\n",
        "            \"pwd\": self.password,\n",
        "            \"agreement_agreement\": 0,\n",
        "            \"is_local\": False\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.post(url, json=payload, headers=headers, timeout=30)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            js = response.json()\n",
        "            if \"data\" not in js or js.get(\"code\") not in (0, 1, 200):\n",
        "                raise RuntimeError(f\"Login falhou: {js}\")\n",
        "\n",
        "            data_to_string = json.dumps(js[\"data\"])\n",
        "            self._token = base64.b64encode(data_to_string.encode(\"utf-8\")).decode(\"utf-8\")\n",
        "            return self._token\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            raise RuntimeError(f\"Erro de conexão durante login: {e}\")\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Erro durante login: {e}\")\n",
        "\n",
        "    def get_inverter_data_by_column(self, inverter_sn: str, column: str, date_str: str) -> Dict[str, Any]:\n",
        "        if not self._token:\n",
        "            raise RuntimeError(\"Token não disponível. Execute login() primeiro.\")\n",
        "\n",
        "        url = f\"{self.BASE_URLS[self.data_region]}/api/PowerStationMonitor/GetInverterDataByColumn\"\n",
        "        headers = {\n",
        "            \"Token\": self._token,\n",
        "            \"Content-Type\": \"application/json\",\n",
        "            \"Accept\": \"*/*\"\n",
        "        }\n",
        "        payload = {\n",
        "            \"date\": date_str,\n",
        "            \"column\": column,\n",
        "            \"id\": inverter_sn\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.post(url, json=payload, headers=headers, timeout=30)\n",
        "            response.raise_for_status()\n",
        "            return response.json()\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            raise RuntimeError(f\"Erro de conexão ao buscar dados: {e}\")\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Erro ao buscar dados: {e}\")\n",
        "\n",
        "class SEMSDataProcessor:\n",
        "    @staticmethod\n",
        "    def parse_column_timeseries(resp_json: Dict[str, Any], column_name: str) -> pd.DataFrame:\n",
        "\n",
        "        def _parse_time(ts):\n",
        "            v = pd.to_datetime(ts, errors='coerce')\n",
        "            if pd.isna(v):\n",
        "                try:\n",
        "                    v = pd.to_datetime(ts, dayfirst=True, errors='coerce')\n",
        "                except:\n",
        "                    v = pd.NaT\n",
        "            return v\n",
        "\n",
        "        items = []\n",
        "        if isinstance(resp_json, dict):\n",
        "            data_obj = resp_json.get('data')\n",
        "            if isinstance(data_obj, dict):\n",
        "                for key in ('column1', 'column2', 'column3', 'items', 'list', 'datas', 'result'):\n",
        "                    if key in data_obj and isinstance(data_obj[key], list):\n",
        "                        items = data_obj[key]\n",
        "                        break\n",
        "\n",
        "            if not items:\n",
        "                for key in ('data', 'items', 'list', 'result', 'datas'):\n",
        "                    if key in resp_json and isinstance(resp_json[key], list):\n",
        "                        items = resp_json[key]\n",
        "                        break\n",
        "\n",
        "        if not items:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        times, values = [], []\n",
        "        for item in items:\n",
        "            if not isinstance(item, dict):\n",
        "                continue\n",
        "\n",
        "            timestamp = (item.get('time') or\n",
        "                        item.get('date') or\n",
        "                        item.get('collectTime') or\n",
        "                        item.get('cTime') or\n",
        "                        item.get('tm'))\n",
        "\n",
        "            if column_name in item:\n",
        "                value = item.get(column_name)\n",
        "            else:\n",
        "                value = (item.get('value') or\n",
        "                        item.get('v') or\n",
        "                        item.get('val') or\n",
        "                        item.get('column'))\n",
        "\n",
        "            if timestamp is None or value is None:\n",
        "                continue\n",
        "\n",
        "            parsed_time = _parse_time(timestamp)\n",
        "            if pd.isna(parsed_time):\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                parsed_value = float(str(value).replace(',', '.'))\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "            times.append(parsed_time)\n",
        "            values.append(parsed_value)\n",
        "\n",
        "        if not times:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        df = pd.DataFrame({'time': times, column_name: values})\n",
        "        return df.dropna().sort_values('time').reset_index(drop=True)\n",
        "\n",
        "\n",
        "def fetch_weekly_sems_data(account: str,\n",
        "                          password: str,\n",
        "                          inverter_sn: str,\n",
        "                          start_date: date,\n",
        "                          columns: List[str],\n",
        "                          login_region: Region = \"us\",\n",
        "                          data_region: Region = \"eu\") -> pd.DataFrame:\n",
        "\n",
        "    print(f\"Buscando dados da semana começando em {start_date}\")\n",
        "\n",
        "    client = SEMSClient(account, password, login_region, data_region)\n",
        "    processor = SEMSDataProcessor()\n",
        "\n",
        "    try:\n",
        "        client.login()\n",
        "        print(\"Login realizado com sucesso!\")\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Erro no login: {e}\")\n",
        "\n",
        "    weekly_data = []\n",
        "\n",
        "    for day_offset in range(7):\n",
        "        current_date = start_date + timedelta(days=day_offset)\n",
        "        date_str = datetime.combine(current_date, dtime(0, 0)).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "        print(f\"\\nDia {day_offset + 1}/7: {current_date}\")\n",
        "\n",
        "        day_dataframes = []\n",
        "        for column in columns:\n",
        "            try:\n",
        "                response = client.get_inverter_data_by_column(inverter_sn, column, date_str)\n",
        "                df_column = processor.parse_column_timeseries(response, column)\n",
        "\n",
        "                if not df_column.empty:\n",
        "                    day_dataframes.append(df_column)\n",
        "                    print(f\"  {column}: {len(df_column)} registros\")\n",
        "                else:\n",
        "                    print(f\"  {column}: sem dados\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  Erro em {column}: {e}\")\n",
        "\n",
        "        if day_dataframes:\n",
        "            day_df = day_dataframes[0]\n",
        "            for df_next in day_dataframes[1:]:\n",
        "                day_df = pd.merge_asof(\n",
        "                    day_df.sort_values(\"time\"),\n",
        "                    df_next.sort_values(\"time\"),\n",
        "                    on=\"time\",\n",
        "                    direction=\"nearest\"\n",
        "                )\n",
        "            weekly_data.append(day_df)\n",
        "\n",
        "    if weekly_data:\n",
        "        final_df = pd.concat(weekly_data, ignore_index=True)\n",
        "        final_df = final_df.sort_values(\"time\").reset_index(drop=True)\n",
        "        print(f\"\\nDados semanais combinados: {len(final_df)} registros\")\n",
        "        return final_df\n",
        "    else:\n",
        "        print(\"Nenhum dado encontrado para a semana!\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "        def main():\n",
        "    ACCOUNT = os.getenv(\"SEMS_ACCOUNT\", \"demo@goodwe.com\")\n",
        "    PASSWORD = os.getenv(\"SEMS_PASSWORD\", \"GoodweSems123!@#\")\n",
        "    INVERTER_SN = \"5010KETU229W6177\"\n",
        "    START_DATE = date(2025, 9, 14)\n",
        "    COLUMNS = [\"Pac\", \"Eday\", \"Cbattery1\", \"Vbattery1\", \"Ipv1\", \"Ipv2\", \"Vpv1\", \"Vpv2\"]\n",
        "\n",
        "    try:\n",
        "        df = fetch_weekly_sems_data(\n",
        "            account=ACCOUNT,\n",
        "            password=PASSWORD,\n",
        "            inverter_sn=INVERTER_SN,\n",
        "            start_date=START_DATE,\n",
        "            columns=COLUMNS,\n",
        "            login_region=\"us\",\n",
        "            data_region=\"eu\"\n",
        "        )\n",
        "\n",
        "        if not df.empty:\n",
        "            print(f\"\\n=== DADOS DA SEMANA ===\")\n",
        "            print(f\"Período: {df['time'].min()} até {df['time'].max()}\")\n",
        "            print(f\"Total de registros: {len(df)}\")\n",
        "            print(f\"Colunas: {list(df.columns)}\")\n",
        "\n",
        "            filename = f\"sems_weekly_{INVERTER_SN}_{START_DATE.strftime('%Y%m%d')}_week.csv\"\n",
        "            df.to_csv(filename, index=False)\n",
        "            print(f\"\\nDados salvos em: {filename}\")\n",
        "\n",
        "            df['date'] = df['time'].dt.date\n",
        "            daily_stats = df.groupby('date').agg({\n",
        "                'Pac': ['mean', 'max', 'sum'],\n",
        "                'Eday': 'max'\n",
        "            }).round(2)\n",
        "\n",
        "            print(\"\\n=== ESTATÍSTICAS DIÁRIAS ===\")\n",
        "            print(daily_stats)\n",
        "\n",
        "        else:\n",
        "            print(\"Nenhum dado foi encontrado!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Erro: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "df_energy = pd.read_csv('/content/sems_weekly_5010KETU229W6177_20250914_week.csv')\n",
        "df_energy.head()\n",
        "\n",
        "\n",
        "df_energy = pd.read_csv('/content/sems_weekly_5010KETU229W6177_20250914_week.csv')\n",
        "df_energy.head()\n",
        "\n",
        "df_energy['Pdc'] = (df_energy['Ipv1'] * df_energy['Vpv1']) + (df_energy['Ipv2'] * df_energy['Vpv2'])\n",
        "df_energy['time'] = pd.to_datetime(df_energy['time']) # Convert to datetime objects\n",
        "df_energy['time_diff_h'] = df_energy['time'].diff().dt.total_seconds().div(3600).fillna(0)\n",
        "df_energy['Ecalc_Wh'] = df_energy['Pdc'] * df_energy['time_diff_h']  # Wh no intervalo\n",
        "\n",
        "df_energy[\"week_day\"] = df_energy[\"time\"].dt.day_name()\n",
        "\n",
        "def combinar_bancos_dados(arquivo1, arquivo2):\n",
        "\n",
        "    # Ensure 'time' column is datetime type in both dataframes\n",
        "    arquivo1['time'] = pd.to_datetime(arquivo1['time'])\n",
        "    arquivo2['time'] = pd.to_datetime(arquivo2['time'])\n",
        "\n",
        "    # Selecionar colunas específicas (ajuste conforme suas necessidades)\n",
        "    colunas_selecionadas_df1 = ['time','week_day', 'Pac', 'Eday', 'Cbattery1','Vbattery1','Pdc','Ecalc_Wh']\n",
        "    colunas_selecionadas_df2 = ['time', 'Global_active_power', 'Global_reactive_power', 'Voltage','Global_intensity','Sub_metering_1','Sub_metering_2','Sub_metering_3', 'Sub1_on','Sub2_on','Sub3_on']\n",
        "\n",
        "    # Filtrar colunas\n",
        "    df1_filtrado = arquivo1[colunas_selecionadas_df1]\n",
        "    df2_filtrado = arquivo2[colunas_selecionadas_df2]\n",
        "\n",
        "    # Combinar os dados usando a coluna 'time'\n",
        "    df_combinado = pd.merge(df1_filtrado, df2_filtrado, on='time', how='inner')\n",
        "\n",
        "    # Salvar resultado\n",
        "    filename = \"dados_combinados.csv\"\n",
        "    df_combinado.to_csv(filename, index=False)\n",
        "    print(f\"Arquivo '{filename}' criado com sucesso!\")\n",
        "    print(f\"Total de registros: {len(df_combinado)}\")\n",
        "\n",
        "    display(df_combinado.head())\n",
        "\n",
        "# Usar a função\n",
        "combinar_bancos_dados(df_energy, df_consumption)\n",
        "\n",
        "df_combinado = pd.read_csv('/content/dados_combinados.csv')\n",
        "\n",
        "pip install influxdb3-python\n",
        "pip install influxdb-client\n",
        "!pip install influxdb_client influxdb3-python requests pandas\n",
        "\n",
        "import os, time\n",
        "from influxdb_client_3 import InfluxDBClient3, Point\n",
        "from influxdb_client import InfluxDBClient\n",
        "\n",
        "\n",
        "bucket = \"projetochallenge\"\n",
        "token = os.environ.get(\"h6nAFNWUsFAE_sWEEZvSq76e_6huFuPMgCbIbsBH7YE2QtGxbKEY3mkC2o0jeobv\")\n",
        "org = \"projetoChallenge\"\n",
        "host = \"https://us-east-1-1.aws.cloud2.influxdata.com\"\n",
        "\n",
        "client = InfluxDBClient3(host=host, token=token, org=org)\n",
        "\n",
        "def enviar_para_influxdb(df_combinado, token, host, org, bucket):\n",
        "    try:\n",
        "        from influxdb_client_3 import InfluxDBClient3, Point, WriteOptions\n",
        "\n",
        "        client = InfluxDBClient3(host=host, token=token, org=org, database=bucket)\n",
        "\n",
        "        # Converter time para datetime\n",
        "        df_combinado['time'] = pd.to_datetime(df_combinado['time'])\n",
        "\n",
        "        print(f\"Enviando {len(df_combinado)} registros para InfluxDB...\")\n",
        "\n",
        "        # InfluxDBClient3 usa write() diretamente com DataFrames\n",
        "        client.write(\n",
        "            record=df_combinado,\n",
        "            data_frame_measurement_name='Ecalc_Wh',\n",
        "            data_frame_tag_columns=['week_day', 'Sub1_on', 'Sub2_on', 'Sub3_on'],\n",
        "            data_frame_timestamp_column='time'\n",
        "        )\n",
        "\n",
        "        print(\"Dados importados para InfluxDB com sucesso!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao enviar para InfluxDB: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "    finally:\n",
        "        if 'client' in locals():\n",
        "            client.close()\n",
        "\n",
        "\n",
        "\n",
        "token = os.getenv(\"INFLUXDB_TOKEN\")\n",
        "os.environ['INFLUXDB_TOKEN'] = 'h6nAFNWUsFAE_sWEEZvSq76e_6huFuPMgCbIbsBH7YE2QtGxbKEY3mkC2o0jeobv-w_j-NAElRQ5KQttMkM04A=='\n",
        "os.environ['INFLUXDB_BUCKET'] = 'projetoChallenge'\n",
        "if token:\n",
        "    try:\n",
        "        enviar_para_influxdb(\n",
        "            df_combinado=df_combinado,\n",
        "            token=token,\n",
        "            host=\"https://us-east-1-1.aws.cloud2.influxdata.com\",\n",
        "            org=\"4211097531c5837f\",\n",
        "            bucket=\"Prototipo Goodwe\"\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Não foi possível enviar para InfluxDB: {e}\")\n",
        "else:\n",
        "    print(\"Token do InfluxDB não configurado. Pulando envio.\")\n",
        "    print(\"Para configurar: os.environ['INFLUXDB_TOKEN'] = 'seu_token'\")\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "WgSW99ewpSJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VgGpoVuLpkER"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}